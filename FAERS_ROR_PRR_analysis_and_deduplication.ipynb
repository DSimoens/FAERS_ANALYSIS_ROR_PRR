{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebad5b8-d229-4770-873e-aa633a34cfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROR and PRR analysis using FAERS database\n",
    "# Version v1.0\n",
    "# Djamilla Simoens\n",
    "\n",
    "# =============================\n",
    "# 1. Install Dependencies\n",
    "# =============================\n",
    "\n",
    "# In Jupyter Notebook, run this cell first:\n",
    "!pip install pandas numpy scipy openpyxl tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64057653-f89b-445e-8c75-0934c83086b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 2. Import The Needed libraries\n",
    "# =============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "from scipy.stats import fisher_exact, chi2_contingency\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd07402-895c-4dbd-ba6b-700a042c694b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 3. Input drug, reaction and timeline of interest\n",
    "# =============================\n",
    "# Type the name of the drug and adverse reactions of interest\n",
    "\n",
    "drug_synonyms = ['alpelisib', 'PIQRAY']\n",
    "event_synonyms = ['Stomatitis']\n",
    "data_dir = './faers_data'  # Update with your folder with the FAERS data, make sure you have .txt files\n",
    "\n",
    "\n",
    "# Define which quartely data you are interested in (e.g., from 2017Q3 to 2025Q1)\n",
    "start_year_q = (2019, 2) # Year and number of quarter\n",
    "end_year_q = (2025, 1) # Year and number of quarter\n",
    "\n",
    "# Log skipped rows info\n",
    "skipped_rows_log = []\n",
    "\n",
    "def is_in_range(fname, start, end):\n",
    "    import re\n",
    "    match = re.search(r'(\\d{2})(Q[1-4])', fname.upper())\n",
    "    if not match:\n",
    "        return False\n",
    "    year_str, quarter_str = match.groups()\n",
    "    year = int('20' + year_str) if int(year_str) < 50 else int('19' + year_str)\n",
    "    quarter = int(quarter_str[1])\n",
    "    return (year, quarter) >= start and (year, quarter) <= end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebadb78d-7f99-4a9e-9da2-c762431c1af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 4. Load FAERS Data\n",
    "# =============================\n",
    "\n",
    "def load_files(folder, pattern, start_range, end_range):\n",
    "    pattern_lower = pattern\n",
    "    pattern_upper = pattern.upper()\n",
    "    files = sorted(glob.glob(os.path.join(folder, pattern_lower)) +\n",
    "                   glob.glob(os.path.join(folder, pattern_upper)))\n",
    "\n",
    "    filtered_files = [f for f in files if is_in_range(os.path.basename(f), start_range, end_range)]\n",
    "\n",
    "    dfs = []\n",
    "    for file in tqdm(filtered_files, desc=f\"Loading {pattern.split('*')[0]}\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file,\n",
    "                sep='$',\n",
    "                dtype=str,\n",
    "                encoding='utf-8',\n",
    "                on_bad_lines='skip',\n",
    "                low_memory=False\n",
    "            )\n",
    "            dfs.append(df)\n",
    "        except UnicodeDecodeError:\n",
    "            try:\n",
    "                df = pd.read_csv(\n",
    "                    file,\n",
    "                    sep='$',\n",
    "                    dtype=str,\n",
    "                    encoding='latin1',\n",
    "                    on_bad_lines='skip',\n",
    "                    low_memory=False\n",
    "                )\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load {file}: {e}\")\n",
    "                skipped_rows_log.append((file, str(e)))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {file}: {e}\")\n",
    "            skipped_rows_log.append((file, str(e)))\n",
    "\n",
    "    return pd.concat(dfs, ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "df_drug = load_files(data_dir, 'DRUG*.txt', start_year_q, end_year_q)\n",
    "df_reac = load_files(data_dir, 'REAC*.txt', start_year_q, end_year_q)\n",
    "print(f\"Total DRUG records: {len(df_drug)}\")\n",
    "print(f\"Total REACTION records: {len(df_reac)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0643e5f-9303-42a9-97a4-ba61157d583f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 5. Deduplication Following VigiMatch Standards\n",
    "# =============================\n",
    "\n",
    "print(\"Filtering suspect drugs...\")\n",
    "df_drug_ps = df_drug[df_drug['role_cod'].str.upper() == 'PS'].copy()\n",
    "df_drug_ps['drugname'] = df_drug_ps['drugname'].fillna('').str.upper()\n",
    "df_reac['pt'] = df_reac['pt'].fillna('').str.upper()\n",
    "\n",
    "# Normalize and deduplicate drug names\n",
    "def normalize_name(name):\n",
    "    if not isinstance(name, str):\n",
    "        return ''\n",
    "    name = name.upper()\n",
    "    for synonym in drug_synonyms:\n",
    "        if synonym.upper() in name:\n",
    "            return synonym.upper()\n",
    "    return name\n",
    "\n",
    "df_drug_ps['drugname_norm'] = df_drug_ps['drugname'].apply(normalize_name)\n",
    "\n",
    "# Apply VigiMatch-style deduplication using following parameters: primaryid, drugname_norm, route, dose_vbm\n",
    "dedup_columns = ['primaryid', 'drugname_norm', 'route', 'dose_vbm']\n",
    "df_drug_ps_before = len(df_drug_ps)\n",
    "df_drug_ps_unique = df_drug_ps.sort_values(by='primaryid').drop_duplicates(subset=dedup_columns)\n",
    "df_drug_ps_after = len(df_drug_ps_unique)\n",
    "duplicates_removed_drug = df_drug_ps_before - df_drug_ps_after\n",
    "print(f\" VigiMatch-style DRUG duplicates removed: {duplicates_removed_drug} (from {df_drug_ps_before} â†’ {df_drug_ps_after})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39af1b6-dbb0-48db-9abe-5250e08b426a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 6. Matching Drug and Event\n",
    "# =============================\n",
    "\n",
    "isr_drug = set()\n",
    "for name in drug_synonyms:\n",
    "    name = name.upper()\n",
    "    matches = df_drug_ps_unique[df_drug_ps_unique['drugname_norm'] == name]['primaryid'].unique()\n",
    "    isr_drug.update(matches)\n",
    "\n",
    "isr_event = set()\n",
    "for name in event_synonyms:\n",
    "    name = name.upper()\n",
    "    matches = df_reac[df_reac['pt'].str.contains(name, na=False, regex=False)]['primaryid'].unique()\n",
    "    isr_event.update(matches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0635afb5-720b-487a-af45-03d32f0c3e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 7. Create Variables for 2x2 Contingency Table\n",
    "# =============================\n",
    "\n",
    "A = len(isr_drug & isr_event)  # Drug + Event\n",
    "B = len(isr_drug - isr_event)  # Drug + No Event\n",
    "C = len(isr_event - isr_drug)  # Event + No Drug\n",
    "all_isr = set(df_drug['primaryid'].unique())\n",
    "D = len(all_isr - (isr_drug | isr_event))  # Neither\n",
    "\n",
    "print(f\"Contingency Table:\\nA: {A}\\nB: {B}\\nC: {C}\\nD: {D}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277b6317-2cbf-49d8-9052-cb18eb046575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 8. Statistical Analysis\n",
    "# =============================\n",
    "\n",
    "# ROR calculation\n",
    "contingency_table = np.array([[A, B], [C, D]])\n",
    "ROR = (A / B) / (C / D) if B > 0 and C > 0 and D > 0 else np.nan\n",
    "\n",
    "# PRR calculation\n",
    "prr_numerator = A / (A + B) if (A + B) > 0 else np.nan\n",
    "prr_denominator = (A + C) / (A + B + C + D) if (A + B + C + D) > 0 else np.nan\n",
    "PRR = prr_numerator / prr_denominator if prr_denominator > 0 else np.nan\n",
    "\n",
    "# Other calculations\n",
    "oddsr, p_fisher = fisher_exact(contingency_table)\n",
    "chi2, p_chi2, dof, expected = chi2_contingency(contingency_table)\n",
    "SE = np.sqrt(1/A + 1/B + 1/C + 1/D) if A*B*C*D > 0 else np.nan\n",
    "lower_CI = np.exp(np.log(ROR) - 1.96 * SE) if not np.isnan(SE) else np.nan\n",
    "upper_CI = np.exp(np.log(ROR) + 1.96 * SE) if not np.isnan(SE) else np.nan\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40beebdc-e484-4a1d-bdaf-38e6113c8d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# 9. Results and Excel Data Output\n",
    "# =============================\n",
    "\n",
    "results = {\n",
    "    'Database': 'FAERS',\n",
    "    'Drug(s)': \", \".join(drug_synonyms),\n",
    "    'Event(s)': \", \".join(event_synonyms),\n",
    "    'Start Quarter': f\"Q{start_year_q[1]} {start_year_q[0]}\",\n",
    "    'End Quarter': f\"Q{end_year_q[1]} {end_year_q[0]}\",\n",
    "    'A (Drug+Event)': A,\n",
    "    'B (Drug+No Event)': B,\n",
    "    'C (Event+No Drug)': C,\n",
    "    'D (Neither)': D,\n",
    "    'ROR': ROR,\n",
    "    'ROR 95% CI Lower': lower_CI,\n",
    "    'ROR 95% CI Upper': upper_CI,\n",
    "    'PRR': PRR,\n",
    "    'Fisher p-value': p_fisher,\n",
    "    'Chi2 p-value': p_chi2,\n",
    "    'DRUG Duplicates Removed': duplicates_removed_drug,\n",
    "    'Original DRUG Records': df_drug_ps_before,\n",
    "    'Skipped Files': len(skipped_rows_log)\n",
    "}\n",
    "\n",
    "result_df = pd.DataFrame([results])\n",
    "print(result_df)\n",
    "\n",
    "output_file = 'FAERS_alpelisib_Term3_ROR_PRR_Results.xlsx'\n",
    "result_df.to_excel(output_file, index=False)\n",
    "print(f\"Results exported to {output_file}\")\n",
    "\n",
    "if skipped_rows_log:\n",
    "    pd.DataFrame(skipped_rows_log, columns=[\"File\", \"Error\"]).to_csv(\"skipped_files_log.csv\", index=False)\n",
    "    print(\"Skipped file errors saved to 'skipped_files_log.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f034e24-ec29-4ea9-964b-1353bc693b8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
